[
  
  {
    "title": "go标准库-sort",
    "url": "/posts/standard_sort/",
    "categories": "go",
    "tags": "go_standard_library",
    "date": "2024-09-09 23:46:12 +0800",
    





    
    "snippet": "该包用于给slice或者用户自定义的集合进行排序排序底层算法go的底层排序算法使用的是pattern-defeat quicksort(pdqSort)算法，PDQSort（Pattern-defeating quicksort）是一种现代的排序算法，它结合了快速排序（Quicksort）、插入排序（Insertion Sort）和堆排序（Heapsort）的优点，并通过模式识别和优化来避免...",
    "content": "该包用于给slice或者用户自定义的集合进行排序排序底层算法go的底层排序算法使用的是pattern-defeat quicksort(pdqSort)算法，PDQSort（Pattern-defeating quicksort）是一种现代的排序算法，它结合了快速排序（Quicksort）、插入排序（Insertion Sort）和堆排序（Heapsort）的优点，并通过模式识别和优化来避免最坏情况的性能退化。go source codepdqSortTypes（类型）sort.Interface​\tsort.Interface是Sort包中的一个接口，该接口定义了3个方法：  Len() int : 返回切片的长度  Less(i,j int) bool:  用于判断切片中两个值的大小  Swap(i,j int): 交换索引i和j处的元素实现该接口的类型可以使用该包的Sort函数进行排序type Interface interface {\t// Len is the number of elements in the collection.\tLen() int\t// Less reports whether the element with index i\t// must sort before the element with index j.\t//\t// If both Less(i, j) and Less(j, i) are false,\t// then the elements at index i and j are considered equal.\t// Sort may place equal elements in any order in the final result,\t// while Stable preserves the original input order of equal elements.\t//\t// Less must describe a transitive ordering:\t//  - if both Less(i, j) and Less(j, k) are true, then Less(i, k) must be true as well.\t//  - if both Less(i, j) and Less(j, k) are false, then Less(i, k) must be false as well.\t//\t// Note that floating-point comparison (the &lt; operator on float32 or float64 values)\t// is not a transitive ordering when not-a-number (NaN) values are involved.\t// See Float64Slice.Less for a correct implementation for floating-point values.\tLess(i, j int) bool\t// Swap swaps the elements with indexes i and j.\tSwap(i, j int)}sort包内置实现sort.Interface接口的类型      Float64Slice        IntSlice        StringSlice    以上三个类型均实现下面的函数：          Len()      Less(i,j int) bool      Search(x) 查找数组中元素，若x在集合中，则返回对应的index，否则返回x应该在集合中的位置      Sort() 对Slice进行排序      Swap(i,j)      例子func SortSlice() {\tarr := []int{1, 2, 4, 5, 61, 13, 2} // 定义一个整形slice\tslice := sort.IntSlice(arr)         // 类型转换\tslice.Sort()                        // 对slice进行排序\tfmt.Println(arr)\tans := slice.Search(12) // 查找数组中元素，若x在集合中，则返回对应的index，否则返回x应该在集合中的位置\tfmt.Println(ans)\tans = slice.Search(2) // 查找数组中元素，若x在集合中，则返回对应的index，否则返回x应该在集合中的位置\tfmt.Println(ans)}Functions  Find函数func Find(n int, cmp func(int) int) (i int, found bool)// 基于binary search 查找在集合[0,n)范围内满足cmp(i) &lt;= 0 最小的索引// 如果查找失败，说明没有满足cmp条件的index，则返回i=n，此时found = false// 如果查找成功，则i&lt;n &amp;&amp; cmp(i) &lt;= 0,此时found = true// cmp函数用于比较待查找的值target与集合x的中元素x[i]的大小,例如:// target &lt; x[i] return &lt;0// target == [i] return =0// target &gt; x[i] return &gt; 0// Examplefunc TestFind(t *testing.T) {\tarr := []int{11, 23, 1, 4, 13}\tfmt.Println(arr)\tslice := sort.IntSlice(arr)\tslice.Sort()\tfmt.Println(arr)\ttarget := 22\tans, found := sort.Find(len(arr), func(i int) int {\t\treturn target - arr[i]\t})\tfmt.Println(found)\tfmt.Println(ans)}  Float64s函数func Float64s(x []float64)// 将float类型的集合升序排序// Examplefunc TestFloat64s(t *testing.T) {\tarr := []float64{1, 2, 34, 4, 2}\tfmt.Println(arr)\tsort.Float64s(arr)\tfmt.Println(arr)}  Float64sAreSorted函数func Float64sAreSorted(x []float64) bool// 判断给定的集合是否已经排序func TestFloat64sAreSorted(t *testing.T) {\tarr := []float64{1, 2, 34, 4, 2}\tfmt.Println(sort.Float64sAreSorted(arr)) // return false\tsort.Float64s(arr)\tfmt.Println(sort.Float64sAreSorted(arr)) // return true}  Ints函数 (同float64s)      IntsAreSorted函数 (同Float64sAreSorted函数)    IsSorted函数func IsSorted(data Interface) bool// 判断sort.Interface类型的集合是否有序// Examplefunc TestIsSorted(t *testing.T) {\tarr := []int{1, 2, 34, 4, 2}\tslice := sort.IntSlice(arr)\tans := sort.IsSorted(slice)\tfmt.Println(ans)\tslice.Sort()\tans = sort.IsSorted(slice)\tfmt.Println(ans)}  Search函数func Search(n int, f func(int) bool) int// Search函数基于binary search 查找[0,n)范围内满足f(i) == true 的最小下标元素// 假设在范围[0, n)内，若f(i)为真，则f(i+1)也为真。// 也就是说，搜索要求函数f对于输入范围[0, n)的某个（可能为空）前缀部分返回假值，然后对其余（可能为空）部分返回真值；搜索将返回第一个使结果为真的索引。// 如果这样的索引不存在，搜索将返回n。（注意：“未找到”的返回值不是-1，例如strings.Index中的情况。）搜索仅对区间[0, n)内的i调用f(i)。// Example func TestSearch(t *testing.T) {\tarr := []int{1, 2, 34, 4, 2}\ttarget := 3\tsort.Ints(arr)\tfmt.Println(arr)\tans := sort.Search(len(arr), func(i int) bool { // 如果存在满足条件的元素，则返回对应元素的下标；否则，return n\t\treturn target &lt;= arr[i]\t})\tif ans &lt; len(arr) &amp;&amp; ans == target { // ans = 3\t\tfmt.Println(ans)\t} else {\t\tfmt.Println(ans)\t}}  SearchFloat64s函数func SearchFloat64s(a []float64, x float64) int// 查找集合中固定值所在的索引// 若x存在于集合中，则返回对应的下标 反之 返回应该插入的位置// 集合必须是升序排列// Example func TestSearchFloat64s(t *testing.T) {\tarr := []float64{1, 2, 34, 4, 2}\tslice := sort.Float64Slice(arr)\tslice.Sort()\tans := sort.SearchFloat64s(arr, 5) // ans = 4\tfmt.Println(ans)}  SearchInts函数（同SearchFloat64）  SearchStrings函数（同SearchFloat64）  Slice函数func Slice(x any, less func(i, j int) bool)// Slice函数按照less函数对集合进行排序// 注意：该方法不是稳定排序// Examplefunc TestSlice(t *testing.T) {\tarr := []float64{1, 2, 34, 4, 2}\tsort.Slice(arr, func(i, j int) bool {\t\treturn arr[i] &lt; arr[j]\t})\tfmt.Println(arr)}  SliceIsSorted函数func SliceIsSorted(x any, less func(i, j int) bool) bool// 判断按照给定less排序方法 集合是否已经有序  SliceStable函数func SliceStable(x any, less func(i, j int) bool)// 稳定的排序算法  Sort函数func Sort(data Interface)// 对实现的sort.Interface的类型集合进行排序// Example type myArr []intfunc (arr myArr) Len() int {\treturn len(arr)}func (arr myArr) Swap(i, j int) {\tarr[i], arr[j] = arr[j], arr[i]}func (arr myArr) Less(i, j int) bool {\treturn arr[i] &lt; arr[j]}func TestSort(t *testing.T) {\tarr := []int{1, 2, 34, 4, 2}\tsort.Sort(myArr(arr))\tfmt.Println(arr)}  Stable函数func Stable(data Interface)// 采用稳定的排序算法  Strings函数func Strings(x []string)// 对字符串进行排序func TestStrings(t *testing.T) {\tarr := []string{\"b\", \"a\"}\tsort.Strings(arr)\tfmt.Println(arr)\tfmt.Println(sort.StringsAreSorted(arr))}  StringsAreSortedfunc StringsAreSorted(x []string) bool// 判断字符串集合是否有序"
  },
  
  {
    "title": "代理和反向代理",
    "url": "/posts/proxy/",
    "categories": "nginx",
    "tags": "nginx",
    "date": "2024-08-23 13:13:12 +0800",
    





    
    "snippet": "代理（Proxy）代理服务器是一个中间服务器，它位于客户端和目标服务器之间，代表客户端向目标服务器发送请求，并将目标服务器的响应返回给客户端。代理服务器的主要功能包括：  隐私保护：代理服务器可以隐藏客户端的真实 IP 地址，从而保护用户隐私。  访问控制：代理服务器可以限制用户访问某些网站或资源。  缓存：代理服务器可以缓存常用资源，从而加快访问速度并减少带宽消耗。  内容过滤：代理服务器...",
    "content": "代理（Proxy）代理服务器是一个中间服务器，它位于客户端和目标服务器之间，代表客户端向目标服务器发送请求，并将目标服务器的响应返回给客户端。代理服务器的主要功能包括：  隐私保护：代理服务器可以隐藏客户端的真实 IP 地址，从而保护用户隐私。  访问控制：代理服务器可以限制用户访问某些网站或资源。  缓存：代理服务器可以缓存常用资源，从而加快访问速度并减少带宽消耗。  内容过滤：代理服务器可以过滤不良内容或广告。假设你在浏览器中访问一个网站，浏览器的请求首先发送到代理服务器，代理服务器再将请求转发给目标网站。目标网站的响应返回给代理服务器，代理服务器再将响应转发给浏览器。反向代理（Reverse Proxy）反向代理服务器也是一个中间服务器，但它位于目标服务器和客户端之间，代表目标服务器接收客户端的请求，并将请求转发给目标服务器。反向代理服务器的主要功能包括：  负载均衡：反向代理服务器可以将客户端的请求分发到多个后端服务器，从而实现负载均衡。  安全性：反向代理服务器可以隐藏后端服务器的真实 IP 地址，从而提高安全性。  SSL 终止：反向代理服务器可以处理 SSL 加密，从而减轻后端服务器的负担。  缓存：反向代理服务器可以缓存后端服务器的响应，从而加快响应速度并减少后端服务器的负载。假设你在浏览器中访问一个网站，浏览器的请求首先发送到反向代理服务器，反向代理服务器再将请求转发给后端服务器。后端服务器的响应返回给反向代理服务器，反向代理服务器再将响应转发给浏览器。复制Client &lt;--&gt; Reverse Proxy &lt;--&gt; Backend Server代理和反向代理的区别  方向：          代理：代理服务器代表客户端向目标服务器发送请求。      反向代理：反向代理服务器代表目标服务器接收客户端的请求。        用途：          代理：主要用于客户端的隐私保护、访问控制、内容过滤和缓存。      反向代理：主要用于负载均衡、安全性、SSL 终止和缓存。      "
  },
  
  {
    "title": "docker入门",
    "url": "/posts/docker%E5%85%A5%E9%97%A8/",
    "categories": "Docker",
    "tags": "Docker",
    "date": "2024-08-23 13:05:12 +0800",
    





    
    "snippet": "docker常用命令docker images # 显示所有容器镜像docker ps -all # 列出所有的docker容器（所有状态）docker pull # 拉取镜像docker start|stop # 启动或者停止镜像docker rm docker_name # 删除容器docker rmi image_name # 删除镜像docker logs -f docker_nam...",
    "content": "docker常用命令docker images # 显示所有容器镜像docker ps -all # 列出所有的docker容器（所有状态）docker pull # 拉取镜像docker start|stop # 启动或者停止镜像docker rm docker_name # 删除容器docker rmi image_name # 删除镜像docker logs -f docker_name # 查看docker容器日志docker run --name container_name image_name # 通过镜像启动容器，并为容器命名docker run -i -t -d -P -p -e key=value docker_name image command # -t 在新容器内制定一个伪终端或终端 -i 允许你对容器内的标准输入进行交互 -d 后台模型运行容器 -P 将容器内部使用的网络端口随机映射到我们使用的主机上 -p 指定映射的端口 -e key=value 设置环境变量docker exec -it docker_name command # 进入容器,使用该命令退出容器时不会导致容器的停止docker commit -m \"commit info\" -a=\"author\" container_id image_name # 创建一个新镜像docker tag Image_id image_name:new_tag容器的状态  created  restarting  running  removing  paused  exited  deaddocker镜像属性  Repository：表示镜像的仓库源  TAG：镜像的标签，如果不指定，默认tag为latest  IMAGE ID：镜像ID  CREATED：镜像创建时间  SIZE：镜像大小Dockerfile  Dockerfile的指令没执行一次都会在docker上新建一层，过多无意义的层，会造成镜像膨胀过大  可以使用&amp;&amp;连接符连接命令，这样只会创建一层镜像# 第一阶段：构建阶段 定制的镜像是基于FROM的镜像，后续的操作都是基于nginx操作FROM golang:1.19 AS builder# 设置工作目录WORKDIR /app# 复制 Go 源码到工作目录, COPY &lt;src&gt; &lt;dest&gt; src是宿主机的目录 dest是镜像的目录COPY . .# 编译 Go 应用程序RUN go build -o hello# 第二阶段：运行阶段 alpine Linux 是一个轻量级的 Linux 发行版镜像FROM alpine:latest # 设置工作目录WORKDIR /app# 从构建阶段复制编译后的二进制文件到运行阶段COPY --from=builder /app/hello .# 运行二进制文件CMD [\"./hello\"]  基于dockerfile创建镜像docker build -t image_name:image_tag context_path # 使用build命令创建镜像# 需要在dockerfile文件存放的目录下 执行该命令# context_path 用于指定上下文的目录，即宿主机的目录，用于传送文件到镜像目录"
  },
  
  {
    "title": "常用shell命令",
    "url": "/posts/%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4/",
    "categories": "Shell",
    "tags": "shell",
    "date": "2024-08-22 15:21:12 +0800",
    





    
    "snippet": "curl命令curl 是一个用于在命令行中进行数据传输的工具，支持多种协议（如 HTTP、HTTPS、FTP 等）。它广泛用于测试和调试网络连接、下载文件、与 API 交互等。以下是一些常见的 curl 命令示例和用法：curl https://example.com # get方式请求数据curl -X post --data 'param=value' https://example.c...",
    "content": "curl命令curl 是一个用于在命令行中进行数据传输的工具，支持多种协议（如 HTTP、HTTPS、FTP 等）。它广泛用于测试和调试网络连接、下载文件、与 API 交互等。以下是一些常见的 curl 命令示例和用法：curl https://example.com # get方式请求数据curl -X post --data 'param=value' https://example.com/ # post方式请求数据curl -X post -H \"Content-Type:application/json\" -d '{\"key1\":\"value1\",\"key1\":\"value1\"}' http://example.com # json 方式请求数据ls命令ls 用于展示文件夹中的内容ls # List files ls -l # list files one per linels -a # list all files,including hidden filesls -f # list all files,with trailing / added to directory names(在目录文件名后面添加'\\')ls -la ls -lh # long format list with size displayed using human-readable unitsls -lSR # long format list sorted by size recursivelyls -ltr # long format list of all files,sorted by modificaiton datels -d */ # only list directoriestail命令tail 用于展示文件后面的一部分内容tail -n count path/to/file  # show last 'count' line in filetail -n +count path/to/file  # show file content from a specific linetail -c count path/to/file # show a specific count of bytes from the end of a given filetail -f path/to/file # keep reading file util crtl + ccat命令cat 用于输出以及链接文件内容cat path/to/file # show the content of a filecat path/to/file path/to/file2 ...&gt; path/to/output_file # concatenate several files into an output file(overwrite the content of the output_file)cat path/to/file path/to/file2 ...&gt;&gt; path/to/output_file # concatenate several files into an output file(append the content of source file to the end of the output_file)cat -u file1 &gt; file2 # copy the contents of file1 to file2（'&gt;' means overwrite,'&gt;&gt;' means append to end of target file）cat - &gt; path/to/file # write stdin to a filegrep命令grep用于按照正则表达式搜索文件内容grep \"search_pattern\" path/to/file # search for a pattern within a filegrep -F|--fixed-strings \"exact_string\" path/to/file # search for an exact string(disabled regular expressions)grep -v|--invert-match \"search_pattern\" # search do not match patternps命令ps用于展示running processes的信息ps aux ｜ -ef # list all processesps -ef | grep string # search a process that matches a stringenv命令env用于查看以及设置环境变量env # show the environment variableenv variable=value # set a variableenv -u variable # remove a variabletop命令top用于查看running processes的动态实时信息top # start topnetstat命令netstat是一个网络统计工具，用于显示与网络相关的各种信息netstat df命令df用于显示文件系统的磁盘空间使用情况df # display all usage of filesystemdf -h # display all usage of filesystem(human readable)lsof命令lsof用于列出当前系统中所有打开的文件lsof # show all opened files,include socket,pipe,device fileslsof -u username # show opened files of this usernamedu命令du用于展示文件占用磁盘空间的量du -h path/to/directory # list the size of a directory and any subdirectoriesid命令id用于显示user和group信息id # show current user infoid username# show specifc user infochown命令chown用于更改文件的所属用户chown user path/to/filechown user:group path/to/filechmod命令chmod用于更改文件的访问权限# - rwx r-- r--# 第一个字符表示文件类型 '-' 表示普通文件 'd' 表示目录 'l'表示符号链接 's'表示套接字 'p'表示命名管道# 接下来的9个字符分为3组，每组3个字符，分别表示owner，group，other的权限# ---------------------# 权限类型# r 表示 读权限# w 表示 写权限# x 表示 执行权限# - 表示 无权限# ---------------------# 操作符# + 表示增加权限# - 表示删除权限# = 表示设置权限# ----------------------# 符号表示法# u 表示 user# g 表示 group# o 表示 others# a 表示 all# ---------------------# 同样也可以使用8进制来表示权限# 1表示执行权限# 2表示写权限# 4表示读权限# 因此上面的文件权限可以表示成# -744chmod u+x  # give the user who owns a file the right to execute it chmod u-x  # remove the user whoe owns a file the right to execute it digdig命令用于dns查询dig example.comhistory```shellhistory # lookup all commands list history with line numbershistory 20 # show the last 20 commands# 如果你不想重新输入过去的命令，可以在命令行号前添加!+line numbersu命令su用于切换用户su # switch to superusersu username # switch to speical usersu - username - C \"command\" # execute a command as another usertcpdumptcpdump用于网络包抓取sudo tcpdump -i eth0 host ipv4 -X # capture the traffic of a specific interface and a host with Ascii formatsudo tcpdump -i eth0 src host port 80 -X # capture the traffic of a specific interface,source host sudo tcpdump -i eth0 dst host port 80 -X # capture the traffic of a specific interface,source host "
  },
  
  {
    "title": "Chapter5-深入Kafka",
    "url": "/posts/Kafka_chpater2kafka_chapter5/",
    "categories": "Kafka",
    "tags": "kafka",
    "date": "2024-07-29 20:15:12 +0800",
    





    
    "snippet": "深入Kafka控制器控制器其实就是一个 broker，只不过它除了具有一般 broker 的功能之外，还负责分区首领的选举。集群里第一个启动的 broker 通过在Zookeeper 里创建一个临时节点 /controller 让自己成为控制器。其他 broker 在启动时也会尝试创建这个节点，不过它们会收到一个“节点已存在”的异常，然后“意识”到控制器节点已存在，也就是说集群里已经有一个控...",
    "content": "深入Kafka控制器控制器其实就是一个 broker，只不过它除了具有一般 broker 的功能之外，还负责分区首领的选举。集群里第一个启动的 broker 通过在Zookeeper 里创建一个临时节点 /controller 让自己成为控制器。其他 broker 在启动时也会尝试创建这个节点，不过它们会收到一个“节点已存在”的异常，然后“意识”到控制器节点已存在，也就是说集群里已经有一个控制器了。复制Kafka 使用主题来组织数据，每个主题被分为若干个分区，每个分区有多个副本。那些副本被保存在 broker 上，每个 broker 可以保存成百上千个属于不同主题和分区的副本。  首领副本（leader）：每个分区都有一个首领副本。为了保证一致性，所有生产者请求和消费者请求都会经过这个副本。  跟随者副本（follower）：首领以外的副本都是跟随者副本。跟随者副本不处理来自客户端的请求，它们唯一的任务就是从首领那里复制消息，保持与首领一致的状态。如果首领发生崩溃，其中的一个跟随者会被提升为新首领。  首选首领：创建主题时选定的首领就是分区的首选首领。之所以把它叫作首选首领，是因为在创建分区时，需要在 broker 之间均衡首。broker请求broker 的大部分工作是处理客户端、分区副本和控制器发送给分区首领的请求。broker通过TCP协议来传输请求。broker 会在它所监听的每一个端口上运行一个 Acceptor 线程，这个线程会创建一个连接，并把它交给 Processor 线程去处理。Processor 线程（也被叫作“网络线程”）的数量是可配置的。网络线程负责从客户端获取请求消息，把它们放进请求队列，然后从响应队列获取响应消息，把它们发送给客户端。所有的请求由首领副本处理。如果 broker 收到一个针对特定分区的请求，而该分区的首领在另一个 broker 上，那么发送请求的客户端会收到一个“非分区首领”的错误响应。当针对特定分区的获取请求被发送到一个不含有该分区首领的 broker上，也会出现同样的错误。Kafka 客户端要自己负责把生产请求和获取请求发送到正确的broker 上。请求格式  Request type  Request version  Correlation ID：个具有唯一性的数字，用于标识请求消息，同时也会出现在响应消息和错误日志里（用于诊断问题）  Client ID请求类型  生产请求：生产者发送的请求  获取请求：消费者或者follower副本发送的请求  元数据请求：获取首领broker信息元数据请求为获取正确的首领broker，客户端需要发送元数据请求来获取分区的首领broker。这种请求包含了客户端感兴趣的主题列表。服务器端的响应消息里指明了这些主题所包含的分区、每个分区都有哪些副本，以及哪个副本是首领。元数据请求可以发送给任意一个 broker，因为所有 broker 都缓存了这些信息。物理存储Kafka 的基本存储单元是分区。分区无法在多个 broker 间进行再细分，也无法在同一个broker 的多个磁盘上进行再细分。"
  },
  
  {
    "title": "Chapter4-消费者",
    "url": "/posts/Kafka_chpater2kafka_chapter4/",
    "categories": "Kafka",
    "tags": "kafka",
    "date": "2024-07-29 20:15:12 +0800",
    





    
    "snippet": "应用程序使用 KafkaConsumer 向 Kafka 订阅主题，并从订阅的主题上接收消息。Kafka消费者通过横向伸缩提升消费者的消费能力Kafka 消费者从属于消费者群组。一个群组里的消费者订阅的是同一个主题，每个消费者接收主题一部分分区的消息。往群组里增加消费者是横向伸缩消费能力的主要方式。Kafka 消费者经常会做一些高延迟的操作，比如把数据写到数据库或 HDFS，或者使用数据进行...",
    "content": "应用程序使用 KafkaConsumer 向 Kafka 订阅主题，并从订阅的主题上接收消息。Kafka消费者通过横向伸缩提升消费者的消费能力Kafka 消费者从属于消费者群组。一个群组里的消费者订阅的是同一个主题，每个消费者接收主题一部分分区的消息。往群组里增加消费者是横向伸缩消费能力的主要方式。Kafka 消费者经常会做一些高延迟的操作，比如把数据写到数据库或 HDFS，或者使用数据进行比较耗时的计算。在这些情况下，单个消费者无法跟上数据生成的速度，所以可以增加更多的消费者，让它们分担负载，每个消费者只处理部分分区的消息，这就是横向伸缩的主要手段。我们有必要为主题创建大量的分区，在负载增长时可以加入更多的消费者。不过要注意，不要让消费者的数量超过主题分区的数量，多余的消费者只会被闲置。第 2 章介绍了如何为主题选择合适的分区数量。消费者群组和分区再均衡  一个新的消费者加入群组时，它读取的是原本由其他消费者读取的消息  当一个消费者被关闭或发生崩溃时，它就离开群组，原本由它读取的分区将由群组里的其他消费者来读取  主题发生变化时，比如管理员添加了新的分区，会发生分区重分配  再均衡：分区的所有权从一个消费者转移到另一个消费者，这样的行为被称为再均衡。在再均衡期间，消费者无法读取消息，造成整个群组一小段时间的不可用。另外，当分区被重新分配给另一个消费者时，消费者当前的读取状态会丢失，它有可能还需要去刷新缓存，在它重新恢复状态之前会拖慢应用程序。创建Kafka消费者消费者属性      必选属性                  bootstrap.servers：指定broker的address，格式为host：post                    key.deserializer：一个类用于反序列化key                    value.deserializer：一个类用于反序列化value                    groun.id:用于指定KafkaConsumer属于那个消费者群组                  其他属性          ` fetch.min.bytes`:该属性指定了消费者从服务器获取记录的最小字节数。broker 在收到消费者的数据请求时，如果可用的数据量小于 fetch.min.bytes 指定的大小，那么它会等到有足够的可用数据时才把它返回给消费者.      fetch.max.wait.ms: feth.max.wait.ms 则 用 于 指 定 broker 的 等 待 时 间， 默 认 是 500ms。      max.partition.fetch.bytes:该属性指定了服务器从每个分区里返回给消费者的最大字节数。它的默认值是 1MB。      session.timeout.ms:该属性指定了消费者在被认为死亡之前可以与服务器断开连接的时间，默认是 3s。如果消费者没有在 session.timeout.ms 指定的时间内发送心跳给群组协调器，就被认为已经死亡，协调器就会触发再均衡，把它的分区分配给群组里的其他消费者。      heartbeat.interval.ms :heartbeat.interval.ms 指 定 了 poll() 方 法 向 协 调 器发送心跳的频率，session.timeout.ms 则指定了消费者可以多久不发送心跳。所以，一般需要同时修改这两个属性，heartbeat.interval.ms 必须比 session.timeout.ms 小，一般 是 session.timeout.ms 的 三 分 之 一。 如 果 session.timeout.ms 是 3s， 那 么 heartbeat.interval.ms 应该是 1s。      auto.offset.reset:该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下（因消费者长时间失效，包含偏移量的记录已经过时并被删除）该作何处理。它的默认值是 latest，意思是说，在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的记录）。另一个值是 earliest，意思是说，在偏移量无效的情况下，消费者将从起始位置读取分区的记录。      enable.auto.commit:该属性指定了消费者是否自动提交偏移量，默认值是 true。      partition.assignment.strategy:根据给定的消费者和主题，决定哪些分区应该被分配给哪个消费者。有两个分配策略：Range和RoundRobin。      client.id:broker 用它来标识从客户端发送过来的消息。      max.poll.records:该属性用于控制单次调用 call() 方法能够返回的记录数量，可以帮你控制在轮询里需要处理的数据量。      receive.buffer.bytes 和 send.buffer.bytes:socket 在读写数据时用到的 TCP 缓冲区也可以设置大小。      读取消息消费者使用poll方法读取消息。每次调用 poll() 方法，它总是返回由生产者写入 Kafka 但还没有被消费者读取过的记录，我们因此可以追踪到哪些记录是被群组里的哪个消费者读取的。  提交：我们把更新分区当前位置的操作叫作提交。  提交的几种方式          自动提交：如果 enable.auto.commit 被设为 true，那么每过 5s，消费者会自动把从 poll() 方法接收到的最大偏移量提交上去。      同步提交：使用 commitSync()提交偏移量最简单也最可靠。      异步提交：使用 commitASync()提交偏移量      同步和异步组合提交      提交特定的偏移量：以上几种方法都是提交poll方法放回数据的最后一个偏移量。      再均衡监听器消费者在退出和进行分区再均衡之前，会做一些清理工作。你会在消费者失去对一个分区的所有权之前提交最后一个已处理记录的偏移量。如果消费者准备了一个缓冲区用于处理偶发的事件，那么在失去分区所有权之前，需要处理在缓冲区累积下来的记录。你可能还需要关闭文件句柄、数据库连接等。在 为 消 费 者 分 配 新 分 区 或 移 除 旧 分 区 时， 可 以 通 过 消 费 者 API 执 行 一 些 应 用 程 序 代码， 在 调 用 subscribe() 方 法 时 传 进 去 一 个 ConsumerRebalanceListener 实 例 就 可 以 了。ConsumerRebalanceListener需要实现两个方法：  ` public void onPartitionsRevoked(Collection partitions) `方法会在再均衡开始之前和消费者停止读取消息之后被调用。如果在这里提交偏移量，下一个接管分区的消费者就知道该从哪里开始读取了。  public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) 方法会在重新分配分区之后和消费者开始读取消息之前被调用。退出读取消息要记住，consumer.wakeup() 是消费者唯一一个可以从其他线程里安全调用的方法。调用 consumer.wakeup() 可以退出 poll()，并抛出 WakeupException 异常，或者如果调用consumer.wakeup() 时线程没有等待轮询，那么异常将在下一轮调用 poll() 时抛出。"
  },
  
  {
    "title": "Chapter3-生产者",
    "url": "/posts/Kafka_chpater2kafka_chapter3/",
    "categories": "Kafka",
    "tags": "kafka",
    "date": "2024-07-29 20:15:12 +0800",
    





    
    "snippet": "kafka第三方客户端第三方客户端通过直接向Kafka网络端口发送适当的字节序列以实现从Kafka读取消息或者写入消息。构建kafka生产者      确定包含的目标topic和发送内容        指定键或分区（可选）        指定key和value的序列化器        发送消息      生产者的属性  必选属性          bootstrap.servers：指定br...",
    "content": "kafka第三方客户端第三方客户端通过直接向Kafka网络端口发送适当的字节序列以实现从Kafka读取消息或者写入消息。构建kafka生产者      确定包含的目标topic和发送内容        指定键或分区（可选）        指定key和value的序列化器        发送消息      生产者的属性  必选属性          bootstrap.servers：指定broker的address，格式为host：post      key.serializer:一个类用于序列化key      value.serializer:一个类用于序列化value            其他属性                  acks: 指定必须有多少个副本收到消息才会认为消息写入成功。                  acks=0表示不等待来自服务器的响应，能够支持最大速度发送消息，吞吐亮高。          acks=1表示收到首领节点的消息便认为消息写入成功。          acks=all表示收到参与复制的全部节点的消息时才认为消息发送成功，这个模式最安全但延迟高。                            buffer.memory:该参数用来设置生产者内存缓冲区的大小，生产者用它缓冲要发送到服务器的消息。如果应用程序发送消息的速度超过发送到服务器的速度，会导致生产者空间不足。                    ` compression.type`:默认情况下，消息发送时不会被压缩。该参数可以设置为 snappy、gzip 或 lz4，它指定了        消息被发送给 broker 之前使用哪一种压缩算法进行压缩。                    retries: 生产者从服务器收到的错误有可能是临时性的错误（比如分区找不到首领）。在这种情况下，retries 参数的值决定了生产者可以重发消息的次数，如果达到这个次数，生产者会放弃重试并返回错误。                    ` batch.size`: 当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算（而不是消息个数）。当批次被填满，批次里的所有消息会被发送出去。不过生产者并不一定都会等到批次被填满才发送，半满        的批次，甚至只包含一个消息的批次也有可能被发送。                    linger.ms: 该参数指定了生产者在发送批次之前等待更多消息加入批次的时间。KafkaProducer 会在批次填满或 linger.ms 达到上限时把批次发送出去。默认情况下，只要有可用的线程，生产者就会把消息发送出去，就算批次里只有一个消息。                    client.id:字符串类型，用于标识client                    max.in.flight.requests.per.connection:该参数指定了生产者在收到服务器响应之前可以发送多少个消息。它的值越高，就会占用越多的内存，不过也会提升吞吐量。                    ` timeout.ms、request.timeout.ms 和 metadata.fetch.timeout.ms`:request.timeout.ms 指定了生产者在发送数据时等待服务器返回响应的时间，metadata.fetch.timeout.ms 指定了生产者在获取元数据（比如目标分区的首领是谁）时等待服务器返回响应的时间。如果等待响应超时，那么生产者要么重试发送数据，要么返回一个错误（抛出异常或执行回调）。timeout.ms 指定了 broker 等待同步副本返回消息确认的时间，与        asks 的配置相匹配——如果在指定时间内没有收到同步副本的确认，那么 broker 就会返回一个错误。                    client.id:该参数指定了在调用 send() 方法或使用 partitionsFor() 方法获取元数据时生产者的阻塞时间。当生产者的发送缓冲区已满，或者没有可用的元数据时，这些方法就会阻塞。在阻塞时间达到 max.block.ms 时，生产者会抛出超时异常。                    max.request.size:该参数用于控制生产者发送的请求大小。它可以指能发送的单个消息的最大值，也可以指        单个请求里所有消息总的大小。                    receive.buffer.bytes 和 send.buffer.bytes:这两个参数分别指定了 TCP socket 接收和发送数据包的缓冲区大小。如果它们被设为-1，        就使用操作系统的默认值。            生产者发送消息的方式  发送并忘记（fire-and-forget）：不关心消息是否能够正常到达broker  同步发送：调用get()方法进行等待方法返回  异步发送：指定回调函数在响应返回时调用消息序列化  使用已有的序列化器和反序列化器（JSON，Avro，Thrift，Protobuf）  使用Avro进行序列化          我们把所有写入数据需要用到的 schema 保存在注册表里，然后在记录里引用 schema 的标识符。负责读取数据的应用程序使用标识符从注册表里拉取 schema 来反序列化记录。序列化器和反序列化器分别负责处理 schema 的注册和拉取。      消息格式一条Kafka消息应该包含以下内容：  目标主题  键（可以为空）：可以用来作为消息的附加信息，也可以用来决定消息被写到主题的那个分区。拥有相同键的消息将被写到同一个分区。如果键值为 null，并且使用了默认的分区器，那么记录将被随机地发送到主题内各个可用的分区上。分区器使用轮询（Round Robin）算法将消息均衡地分布到各个分区上。[注意：只有在不改变主题分区数量的情况下，键与分区之间的映射才能保持不变。]  值"
  },
  
  {
    "title": "Kafka安装",
    "url": "/posts/Kafka_chpater2/",
    "categories": "Kafka",
    "tags": "kafka",
    "date": "2024-07-26 14:53:12 +0800",
    





    
    "snippet": "安装kafka安装依赖      安装java依赖    wget https://builds.openlogic.com/downloadJDK/openlogic-openjdk/8u412-b08/openlogic-openjdk-8u412-b08-linux-x64.tar.gz # 安装jdk8            安装zookeeper依赖    wget https:/...",
    "content": "安装kafka安装依赖      安装java依赖    wget https://builds.openlogic.com/downloadJDK/openlogic-openjdk/8u412-b08/openlogic-openjdk-8u412-b08-linux-x64.tar.gz # 安装jdk8            安装zookeeper依赖    wget https://dlcdn.apache.org/zookeeper/zookeeper-3.8.4/apache-zookeeper-3.8.4-bin.tar.gz      安装kafka  wget https://dlcdn.apache.org/kafka/3.7.1/kafka_2.13-3.7.1.tgz影响kafka性能的因素  磁盘吞吐量-影响生产者（建议选择固态硬盘）  磁盘容量  内存-影响消费者  网络延迟  CPU"
  },
  
  {
    "title": "Kafka基本概念",
    "url": "/posts/Kafka_chpater1/",
    "categories": "Kafka",
    "tags": "kafka",
    "date": "2024-07-26 10:55:12 +0800",
    





    
    "snippet": "基本概念消息和批次  kafka的消息由字节组成，消息里面的数据没有特别的格式或者含义。  效益可以有一个可选的元数据——key（键），用于为消息选择分区。  kafka消息是分批次写入的kafka，这些消息属于用一个主题或者分区。主题和分区  kafka消息通过主题进行分类，一个主题可以分为多个分区，同一个主题的不同分区可以分布在不同的服务器上。消息以FIFO的方式写入分区，以FIFO的方...",
    "content": "基本概念消息和批次  kafka的消息由字节组成，消息里面的数据没有特别的格式或者含义。  效益可以有一个可选的元数据——key（键），用于为消息选择分区。  kafka消息是分批次写入的kafka，这些消息属于用一个主题或者分区。主题和分区  kafka消息通过主题进行分类，一个主题可以分为多个分区，同一个主题的不同分区可以分布在不同的服务器上。消息以FIFO的方式写入分区，以FIFO的方式读取。生产者和消费者      生产者：发布一个消息到特定的主题上，但不关心特定消息会被写到那个分区上。        消费者：消费者订阅一个或多个主题，并按照消息生成的顺序读取他们。并通过一种元数据——偏移量来区分已经读取过的消息。        消费者群组：共同读取同一个主题的消费者构成一个消费者群组。群组保证每个分区只能被一个消费者使用。如下图所示：      broker和集群      broker：一个独立的Kafka服务器被称为broker。broke接收来自生产者的消息，为消息设置偏移量，并提交消息到磁盘保存。        集群控制器：broker是集群的组成部分，每个集群都会选举出一个活跃的broker充当集群控制器的角色，该控制器负责管理工作。        首领：一个分区从属于一个broker，一个分区可以分配给多个broker。            保留消息策略：kafka会将消息保留一段时间或者一定大小的字节数，当超过这个限制时，旧消息会过期并被删除。每个主题可以设置单独的保留策略。  kafaka的优点  支持多个生产者、消费者  基于磁盘的数据存储，允许消费者非实时地读取消息  伸缩性，可以在线拓展集群  高性能，提供亚秒级的消息延迟kafka应用场景  活动跟踪——跟踪网站的用户活动，为机器学习系统提供数据  传递消息  度量指标和日志记录  提交日志  流处理"
  }
  
]

